{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version  2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print('tensorflow version ',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb,info = tfds.load(name='imdb_reviews',as_supervised=True,with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    version=1.0.0,\n",
       "    description='Large Movie Review Dataset.\n",
       "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=100000,\n",
       "    splits={\n",
       "        'test': 25000,\n",
       "        'train': 25000,\n",
       "        'unsupervised': 50000,\n",
       "    },\n",
       "    supervised_keys=('text', 'label'),\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'train': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'unsupervised': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'],imdb['test']\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "test_sentences=[]\n",
    "test_labels=[]\n",
    "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
    "for sentence,label in train_data:\n",
    "    train_sentences.append(str(sentence.numpy()))\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "for sentence,label in test_data:\n",
    "    test_sentences.append(str(sentence.numpy()))\n",
    "    test_labels.append(label.numpy())\n",
    "\n",
    "train_labels_final = np.array(train_labels)\n",
    "test_labels_final = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "# training_sentences = []\n",
    "# training_labels = []\n",
    "\n",
    "# testing_sentences = []\n",
    "# testing_labels = []\n",
    "\n",
    "# # str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
    "# for s,l in train_data:\n",
    "#     training_sentences.append(str(s.numpy()))\n",
    "#     training_labels.append(l.numpy())\n",
    "\n",
    "# for s,l in test_data:\n",
    "#     testing_sentences.append(str(s.numpy()))\n",
    "#     testing_labels.append(l.numpy())\n",
    "\n",
    "  \n",
    "# training_labels_final = np.array(training_labels)\n",
    "# testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"This was an absolutely terrible movie. Don\\'t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\\'s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor\\'s like Christopher Walken\\'s good name. I could barely sit through it.\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length,truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index=dict([(v,k) for (k,v) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i,'?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=6,activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1,activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.4928 - accuracy: 0.7444 - val_loss: 0.3885 - val_accuracy: 0.8208\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2379 - accuracy: 0.9085 - val_loss: 0.4209 - val_accuracy: 0.8152\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0863 - accuracy: 0.9791 - val_loss: 0.5199 - val_accuracy: 0.8035\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0222 - accuracy: 0.9971 - val_loss: 0.6035 - val_accuracy: 0.8058\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0062 - accuracy: 0.9996 - val_loss: 0.6899 - val_accuracy: 0.8032\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.7501 - val_accuracy: 0.8033\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.5215e-04 - accuracy: 1.0000 - val_loss: 0.7999 - val_accuracy: 0.8036\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.5768e-04 - accuracy: 1.0000 - val_loss: 0.8459 - val_accuracy: 0.8044\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6406e-04 - accuracy: 1.0000 - val_loss: 0.8893 - val_accuracy: 0.8048\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.5822e-04 - accuracy: 1.0000 - val_loss: 0.9334 - val_accuracy: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x142fa95f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded, train_labels_final, epochs=num_epochs, validation_data=(testing_padded, test_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = embedding_layer.get_weights()[0]\n",
    "weights.shape # vocab_size, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "vectors = io.open('vecs.tsv','w',encoding='utf-8')\n",
    "meta = io.open('meta.tsv','w',encoding='utf-8')\n",
    "\n",
    "for word_num in range(1,vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    meta.write(word+\"\\n\")\n",
    "    vectors.write(\"\\t\".join([str(x) for x in embeddings])+\"\\n\")\n",
    "vectors.close()\n",
    "meta.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
