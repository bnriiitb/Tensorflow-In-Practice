{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version  2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print('tensorflow version ',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb,info = tfds.load(name='imdb_reviews',as_supervised=True,with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    version=1.0.0,\n",
       "    description='Large Movie Review Dataset.\n",
       "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=100000,\n",
       "    splits={\n",
       "        'test': 25000,\n",
       "        'train': 25000,\n",
       "        'unsupervised': 50000,\n",
       "    },\n",
       "    supervised_keys=('text', 'label'),\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'train': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'unsupervised': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'],imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "test_sentences=[]\n",
    "test_labels=[]\n",
    "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
    "for sentence,label in train_data:\n",
    "    train_sentences.append(str(sentence.numpy()))\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "for sentence,label in test_data:\n",
    "    test_sentences.append(str(sentence.numpy()))\n",
    "    test_labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_final = np.array(train_labels)\n",
    "test_labels_final = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"This was an absolutely terrible movie. Don\\'t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\\'s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor\\'s like Christopher Walken\\'s good name. I could barely sit through it.\"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "embedding_dim = 16\n",
    "max_length=120\n",
    "trunc_type='post'\n",
    "oov_token = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_sequences = pad_sequences(train_sequences,maxlen=max_length,truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test_sequences = pad_sequences(test_sequences,maxlen=max_length,truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,   59,   12,   14,   35,  439,  400,   18,  174,   29,\n",
       "          1,    9,   33, 1378, 3401,   42,  496,    1,  197,   25,   88,\n",
       "        156,   19,   12,  211,  340,   29,   70,  248,  213,    9,  486,\n",
       "         62,   70,   88,  116,   99,   24, 5740,   12, 3317,  657,  777,\n",
       "         12,   18,    7,   35,  406, 8228,  178, 2477,  426,    2,   92,\n",
       "       1253,  140,   72,  149,   55,    2,    1, 7525,   72,  229,   70,\n",
       "       2962,   16,    1, 2880,    1,    1, 1506, 4998,    3,   40, 3947,\n",
       "        119, 1608,   17, 3401,   14,  163,   19,    4, 1253,  927, 7986,\n",
       "          9,    4,   18,   13,   14, 4200,    5,  102,  148, 1237,   11,\n",
       "        240,  692,   13,   44,   25,  101,   39,   12, 7232,    1,   39,\n",
       "       1378,    1,   52,  409,   11,   99, 1214,  874,  145,   10],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index=dict([(v,k) for (k,v) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i,'?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? ? b this was an absolutely terrible movie don't be <OOV> in by christopher walken or michael <OOV> both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie's ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the <OOV> rebels were making their cases for <OOV> maria <OOV> <OOV> appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining <OOV> like christopher <OOV> good name i could barely sit through it\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(padded_train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"This was an absolutely terrible movie. Don\\'t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\\'s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor\\'s like Christopher Walken\\'s good name. I could barely sit through it.\"'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                38420     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 198,441\n",
      "Trainable params: 198,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=20,activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1,activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.4753 - accuracy: 0.7577 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2185 - accuracy: 0.9174 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0624 - accuracy: 0.9849 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0118 - accuracy: 0.9986 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.2301e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.9876e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2416e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.3601e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.2530e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.1447e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.1968e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.0336e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.2980e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.3140e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 5.2955e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.3718e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2073e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.4101e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 9.1878e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14dca9080>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "model.fit(padded_train_sequences,train_labels_final,epochs=num_epochs,validation_data=[padded_test_sequences,test_labels_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = embedding_layer.get_weights()[0]\n",
    "weights.shape # vocab_size, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = io.open('vecs.tsv','w',encoding='utf-8')\n",
    "meta = io.open('meta.tsv','w',encoding='utf-8')\n",
    "\n",
    "for word_num in range(1,vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    meta.write(word+\"\\n\")\n",
    "    vectors.write(\"\\t\".join([str(x) for x in embeddings])+\"\\n\")\n",
    "vectors.close()\n",
    "meta.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
